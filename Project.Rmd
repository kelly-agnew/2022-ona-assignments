---
title: "Project"
output: 
  pdf_document: 
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## Libraries
```{r}
library(ggraph)
library(igraph)

library(arrow)
library(tidyverse)
library(gender)
library(wru)
library(lubridate)

library(ggplot2)
library(gridExtra)
library(grid)

library(stargazer)
```

## Data
```{r}
data_path <- "Data/"
applications <- read_parquet(paste0(data_path,"app_data_sample.parquet"))
edges <- read_csv(paste0(data_path,"edges_sample.csv"))
```

## Add gender
```{r}
# get a list of first names without repetitions
examiner_names <- applications %>% 
  distinct(examiner_name_first)

# get a table of names and gender
examiner_names_gender <- examiner_names %>% 
  do(results = gender(.$examiner_name_first, method = "ssa")) %>% 
  unnest(cols = c(results), keep_empty = TRUE) %>% 
  select(
    examiner_name_first = name,
    gender,
    proportion_female
  )
# remove extra columns from the gender table
examiner_names_gender <- examiner_names_gender %>% 
  select(examiner_name_first, gender)

# joining gender back to the dataset
applications <- applications %>% 
  left_join(examiner_names_gender, by = "examiner_name_first")

# cleaning up
rm(examiner_names)
rm(examiner_names_gender)
gc()

```

## Add race
```{r}
# get list of distinct last names
examiner_surnames <- applications %>% 
  select(surname = examiner_name_last) %>% 
  distinct()

examiner_race <- predict_race(voter.file = examiner_surnames, surname.only = T) %>% 
  as_tibble()

# infer racial probabilities from surname tibble
examiner_race <- examiner_race %>% 
  mutate(max_race_p = pmax(pred.asi, pred.bla, pred.his, pred.oth, pred.whi)) %>% 
  mutate(race = case_when(
    max_race_p == pred.asi ~ "Asian",
    max_race_p == pred.bla ~ "black",
    max_race_p == pred.his ~ "Hispanic",
    max_race_p == pred.oth ~ "other",
    max_race_p == pred.whi ~ "white",
    TRUE ~ NA_character_
  ))

# removing extra columns and merge into application data
examiner_race <- examiner_race %>% 
  select(surname,race)

applications <- applications %>% 
  left_join(examiner_race, by = c("examiner_name_last" = "surname"))
# cleanup
rm(examiner_race)
rm(examiner_surnames)
gc()
```

## Add tenure
```{r}
# get all application filing dates
examiner_dates <- applications %>% 
  select(examiner_id, filing_date, appl_status_date) 

# calculate start and end date from filing / status date respectively
examiner_dates <- examiner_dates %>% 
  mutate(start_date = ymd(filing_date), end_date = as_date(dmy_hms(appl_status_date)))

# for each examiner, get earliest and latest days, then interval between them as tenure in days
examiner_dates <- examiner_dates %>% 
  group_by(examiner_id) %>% 
  summarise(
    earliest_date = min(start_date, na.rm = TRUE), 
    latest_date = max(end_date, na.rm = TRUE),
    tenure_days = interval(earliest_date, latest_date) %/% days(1)
    ) %>% 
  filter(year(latest_date)<2018)

# merge and clean
applications <- applications %>% 
  left_join(examiner_dates, by = "examiner_id")

rm(examiner_dates)
gc()
```

## Add application duration
```{r}
# Since an application can only be issued or abandoned, one or the other will always be NA, therefore I will combine the columns

applications$appl_end_date <- paste(applications$patent_issue_date, applications$abandon_date, sep=',')

# Then I will clean up the column by removing instances of commas and NA's
applications$appl_end_date <- gsub('NA', "", as.character(applications$appl_end_date))
applications$appl_end_date <- gsub(',', "", as.character(applications$appl_end_date))

# Ensure date format is consistent for both columns
applications$appl_end_date <- as.Date(applications$appl_end_date, format="%Y-%m-%d")
applications$filing_date <- as.Date(applications$filing_date, format="%Y-%m-%d")

# Finding the difference in days between the application end date and the filing date
applications$appl_proc_days <- as.numeric(difftime(applications$appl_end_date, applications$filing_date, units=c("days")))

# Remove instances where the filing date happens after the issue or abandon dates (these must be mistakes as this shouldnt be possible
applications <- applications %>% filter(appl_proc_days >=0 & !is.na(appl_proc_days))

gc()
```

Check completeness of the dataset to this point

```{r}
library(skimr)
applications %>% skim()
```
Given that our goal is to measure the relationship between centrality and application processing time, there are a few variables here that may be worth imputing to remove NaNs.

- Gender
- tenure days
- appl days

We will use R's mice package which performs multiple imputation under the assumption that any missing data is 'Missing At Random' ie the probability that a value is missing depends only on the observed value itself. Mice will impute data for each input variable by specifying a unique imputation model per-variable. Ie if our feature set consists of X1, X2, ... Xn and X1 has missing values, it will be imputed based on the patterns observed in X2....Xn.

Before we do this, we have to remove some variables which may be missing not-at-random, or are deemed to be unhelpful for the later modelling stage. 

```{r}
applications_subs = subset(applications, select=-c(examiner_name_middle,patent_number, appl_status_date,patent_issue_date,abandon_date,earliest_date,latest_date, filing_date))
# Removal explanations:

# some people might not have a middle name by choice (ie it was not just randomly forgotten to be entered into the data base)
# missing patent number means no patent issues, not missing at random
# appl_status_date for the same reason as patent number, and all of the related date-measurements arising from this

# we remove the remaining date columns since we already have the metrics we need from them (tenure and application processing time), # and mice does not play well with date columns

# we want examiner_id to remain unique which will not be the case if we allow mice to impute it, so we have no choice but to drop the missing examinerid rows
applications_subs = applications_subs %>% drop_na(examiner_id)

```

```{r}
library(mice)
md.pattern(applications_subs)
# there are 1696847 observations with no missing values (84% of the dataset)
# another 14% has just one missing value (gender)
# the remaining 2% of missing values is composed of the other features

applications_subs$gender = as.factor(applications_subs$gender) # mice will only impute on categorically-defined columns and numericals

applications_full = complete(mice(applications_subs, m=3, maxit=3)) # impute using default mice imputation, 3 iterations (mice will decide the appropriate model for each column). This will be ~5-10 minutes
rm(applications_subs)

applications_full %>% skim() # all done
```

With our remaining values imputed, we can proceed with constructing our advice network and calculating centralities

## Advice networks
```{r}
# first get work group for each examiner and limit to our two wgs of interest
examiner_aus = distinct(subset(applications_full, select=c(examiner_art_unit, examiner_id)))
# we eventually want to make a network with nodes colored by work group, so lets add that indicator
examiner_aus$wg = substr(examiner_aus$examiner_art_unit, 1,3)
# restrict down to our selected art units to reduce merging complexity later on
# examiner_aus = examiner_aus[examiner_aus$wg==163 | examiner_aus$wg==176,]

# now we will merge in the aus df on applications 
adviceNet = merge(x=edges, y=examiner_aus, by.x="ego_examiner_id", by.y="examiner_id", all.x=TRUE)
adviceNet = adviceNet %>% rename(ego_art_unit=examiner_art_unit, ego_wg=wg)

# drop edges which are missing ego or alter id
adviceNet = drop_na(adviceNet)

# now repeat for the alter examiners
adviceNet = merge(x=adviceNet, y=examiner_aus, by.x="alter_examiner_id", by.y="examiner_id", all.x=TRUE)
adviceNet = adviceNet %>% rename(alter_art_unit=examiner_art_unit, alter_wg=wg)
adviceNet = drop_na(adviceNet)

egoNodes = subset(adviceNet, select=c(ego_examiner_id,ego_art_unit, ego_wg)) %>%   rename(examiner_id=ego_examiner_id,art_unit=ego_art_unit,wg=ego_wg)
alterNodes = subset(adviceNet, select=c(alter_examiner_id,alter_art_unit, alter_wg))%>% rename(examiner_id=alter_examiner_id,art_unit=alter_art_unit,wg=alter_wg)
nodes = rbind(egoNodes, alterNodes)
nodes = distinct(nodes) #5412 examiners(but some are repeated because they move amongst art units)

# when we reduce to the list of distinct vertices, we actually have more than we should, since some examiners move amongst art units/wgs in this data subset
nodes = nodes %>% group_by(examiner_id) %>% summarise(examiner_id=first(examiner_id), art_unit=first(art_unit), wg=first(wg))
# we are left with just 2400 unique examiners
```

## Construct network and calculate centralities
```{r}
adviceNet = graph_from_data_frame(d=adviceNet, vertices=nodes, directed=TRUE)
# centralities
Degree <- degree(adviceNet, v=V(adviceNet))
Betweenness <- betweenness(adviceNet)
Eigenvector <- evcent(adviceNet)$vector

V(adviceNet)$size = Degree
V(adviceNet)$eig = round(Eigenvector,2)
V(adviceNet)$bet = round(Betweenness,2)


```

## Model the relationship between centralities and app_proc_time
```{r}
# first we'll need to merge the centrality measurements back into the imputed applications set
centralities <- cbind(Degree, Eigenvector, Betweenness)
centralities = round(centralities,2)
centralities = data.frame(centralities)
centralities <- cbind(examiner_id = rownames(centralities), centralities)
rownames(centralities) <- 1:nrow(centralities)

centralities %>% skim() # no missing values but very heavily skewed towards 0 for all centrality measures

# now merge on examiner_id
applications_final = merge(x=applications_full, y=centralities, by="examiner_id", all.x=TRUE)
applications_final %>% skim() # we will have quite a few NaNs popping back up for those examiners who didnt ask any advice
# nothing to do there but remove the missing values
applications_final = drop_na(applications_final)

# clean
rm(examiner_aus)
rm(egoNodes)
rm(alterNodes)
rm(nodes)
rm(adviceNet)
gc()

```

## Modelling
```{r}
# we wish to model the relationship between various centralities and appl_days
# we will make our first model as a simplistic model assuming no interactions among predictors
lm1 = lm(appl_proc_days~Degree+Eigenvector+Betweenness+tenure_days+gender+race, data=applications_final)
summary(lm1)



```
Interpretations:
- The "baseline" expectation for application processing time is 1500
-     That would be for a female asian examiner who just started, 0 tenure days, and has never asked any advice

- Everytime an examiner asks advice to a new colleague examiner (increase degree by 1), we expect processing time to increase slightly (.07 days)
- Increasing an examiner's importance as measured by eigenvector centrality is expected to decrease processing time by 160 days
- Increasing an examiner's betweenness increases the processing time slightly (less than a day)

- It is important to note that the centrality measurements are all coupled, so in a vacuum these interpretations are valid, but in practice we could not increase an examiner's degree without also altering in some way their eigenvector and betweenness centralities

- Longer tenured examiners process applications a bit faster with each additional day of tenure
- Male examiners are expected to take roughly 2 weeks longer than their female counterparts
- Black, Hispanic, and Other-raced examiners all take longer to process than asian
- White examiners process applications much faster than Asian, by about 60 days

- Important to note the out goodness of fit is very low, so these insights should be taken with a grain of salt

We can try to capture some of the more complex relationships among predictors by adding interactions
```{r}

lm2 = lm(appl_proc_days~Degree+Eigenvector+Betweenness+tenure_days+gender+race+Degree*gender+Eigenvector*gender+Betweenness*gender+Degree*race+Eigenvector*race+Betweenness*race, data=applications_final)
summary(lm2)


# stargazer(lm1, lm2, 
#           type="latex",
#           dep.var.labels = "Application Processing Time",
#           covariate.labels= c("Degree Centrality", "Eigenvector Centrality", "Betweenness Centrality", "Tenure (days)", "Male", "Black", "Hispanic", "Other", "White", "Degree:Male", "Eigenvector:Male", "Betweenness:Male","Degree:Black","Degree:Hispanic","Degree:Other","Degree:White","Eigenvector:Black","Eigenvector:Hispanic","Eigenvector:Other","Eigenvector:White","Betweenness:Black","Betweenness:Hispanic","Betweenness:Other","Betweenness:White"),
#           digits = 2)


```

- The baseline expectation is roughly the same as it was before, around 1500 days

- Increasing degree or betweenness centrality (in a vaccuum) is expected to increase processing time, while increasing eigenvector centrality decreases processing time quite significantly (4500 days)
- This model expects Black examiners to process faster than asian examiners, and every other race to be slower

- From interaction terms, we also know that increasing degree for male examiners decreases processing time significantly
- Male examiners with higher betweenness centrality have roughly the same expected processing time (0.005 days added)

Disclaimer: While these models are providing theoretically meaningful insights, we should note that the proportion of variance in the data explained by both of these models is around 1%, ie they are not particularly good models as far as goodness-of-fit is concerned. 


## Workgroup-specific analysis
After completing the general USPTO analysis, we have chosen to zoom in on two tech units: 1600 and 2100. We wanted to look at the STEM field and specifically the differences between life-science related patents (1600: Biotech and Organic Fields) and compute-science related patents (2100: Computer Architecture and Information Security)

We will use workgroups 162 and 219 as the representative work groups for these two tech units, and randomly sample from the larger workgroup to get two approximately evenly sized workgroup data sets. 
```{r}
# first get work group for each examiner and limit to our two wgs of interest
examiner_aus = distinct(subset(applications_full, select=c(examiner_art_unit, examiner_id)), examiner_id, .keep_all=TRUE)

# note we want distinct examiners, not just distinct art_unit+examiner combos, since examiners can move between units. In this case we just take the first art unit an examiner worked in to simplify our analysis

# we eventually want to make a network with nodes colored by work group, so lets add that indicator
examiner_aus$wg = substr(examiner_aus$examiner_art_unit, 1,3)
# restrict down to our selected art units to reduce merging complexity later on
examiner_aus = examiner_aus[examiner_aus$wg==162 | examiner_aus$wg==219,]

#examiner_aus %>% count(wg) # we have a few extra examiners in 162, so randomly sample down to 174 to get even sample sizes
#examiner_aus = rbind(sample_n(examiner_aus[examiner_aus$wg==162,], 174), examiner_aus[examiner_aus$wg==219,])
#examiner_aus %>% count(wg)



# now we will merge in the aus df on applications 
adviceNet = merge(x=edges, y=examiner_aus, by.x="ego_examiner_id", by.y="examiner_id", all.x=TRUE)
adviceNet = adviceNet %>% rename(ego_art_unit=examiner_art_unit, ego_wg=wg)

# drop edges which are missing ego or alter id
adviceNet = drop_na(adviceNet)

# now repeat for the alter examiners
adviceNet = merge(x=adviceNet, y=examiner_aus, by.x="alter_examiner_id", by.y="examiner_id", all.x=TRUE)
adviceNet = adviceNet %>% rename(alter_art_unit=examiner_art_unit, alter_wg=wg)
adviceNet = drop_na(adviceNet)

egoNodes = subset(adviceNet, select=c(ego_examiner_id,ego_art_unit, ego_wg)) %>%   rename(examiner_id=ego_examiner_id,art_unit=ego_art_unit,wg=ego_wg)
alterNodes = subset(adviceNet, select=c(alter_examiner_id,alter_art_unit, alter_wg))%>% rename(examiner_id=alter_examiner_id,art_unit=alter_art_unit,wg=alter_wg)
nodes = rbind(egoNodes, alterNodes)
nodes = distinct(nodes) 

# note we have fewer examiners than we started with due to some examiners never asking each other for advice


# when we reduce to the list of distinct vertices, we actually have more than we should, since some examiners move amongst art units/wgs in this data subset
#nodes = nodes %>% group_by(examiner_id) %>% summarise(examiner_id=first(examiner_id), art_unit=first(art_unit), wg=first(wg))

```

## Repeat centralities analysis

### Construct network and calculate centralities
```{r}
adviceNet = graph_from_data_frame(d=adviceNet, vertices=nodes, directed=TRUE)
# centralities
Degree <- degree(adviceNet, v=V(adviceNet))
Betweenness <- betweenness(adviceNet)
Eigenvector <- evcent(adviceNet)$vector

V(adviceNet)$size = Degree
V(adviceNet)$eig = round(Eigenvector,2)
V(adviceNet)$bet = round(Betweenness,2)
V(adviceNet)$wg = nodes$wg

```

### Visualize
```{r}
ggraph(adviceNet, layout="kk") +
  geom_edge_link()+
  geom_node_point(aes(size=size, color=wg), show.legend=T)

```
We have a much sparser network here with many components instead of one or two large components. This is likely due to the restrictive size of our analysis, however it is still interesting to note the existence of these cliques, especially given that for some examiners we have 15-20 instances of advice asking. This shows a clear preference amongst the examiners in both 162 and 219 to stick with their local friend group when resolving issues.


### Model the relationship between centralities and app_proc_time
```{r}
# first we'll need to merge the centrality measurements back into the imputed applications set
centralities <- cbind(Degree, Eigenvector, Betweenness)
centralities = round(centralities,2)
centralities = data.frame(centralities)
centralities <- cbind(examiner_id = rownames(centralities), centralities)
rownames(centralities) <- 1:nrow(centralities)

centralities = merge(centralities, subset(examiner_aus, select=-c(wg)), by="examiner_id") # need art unit for the final merge with applications

# now merge on examiner_id
applications_final = merge(x=applications_full, y=centralities, by=c("examiner_id","examiner_art_unit"), all.y=TRUE)
applications_final %>% skim() # we will have quite a few NaNs popping back up for those examiners who didnt ask any advice
# nothing to do there but remove the missing values
applications_final = drop_na(applications_final)


```

### Modelling
```{r}
applications_final$wg = substr(applications_final$examiner_art_unit,1,3)
applications_final$wg = as.factor(applications_final$wg)

unique(applications_final$race) # Just Asian, White, or Hispanic examiners present in this dataset



# for our first model we will once again cover no interactions and just look at base variables
# also, we have dropped betweenness because it is 0 for all examiners, probably due to the lack of connectivity between clusters
lm1 = lm(appl_proc_days~Degree+Eigenvector+tenure_days+gender+race+wg, data=applications_final)
summary(lm1)
```
Our work-group specific analysis gives much different results from before

First, our baseline estimate (Female, Asian, 0 tenure days and no prior connections) for application time is 1038 days

In addition, we assume a further increase in processing time for each advice-sought by about 20 days

One notable insight is that examiners from work group 219 are expected to take significantly longer in processing applications than for those in workgroup 162. This could potentially be due to the larger size of workgroup 162, allowing for lower on-average workload. It is also possible the discrepency is due to a simple difference in the nature/complexity of Biotech vs CS -oriented patents. 

We also expect Hispanic and White examiners to complete applications faster than Asian examiners.

Based on this simplistic model, we would naively conclude that the USPTO should focus on hiring more Hispanic and White female examiners, as we expect them to process applications much faster than all male examiners, and especially faster than male asian examiners.

Of course, we know this model is missing the whole picture and we ought to increase its complexity before making conclusions...
```{r}
# add interactions
lm2 = lm(appl_proc_days~Degree+Eigenvector+tenure_days+gender+race+wg
         +gender*Degree+race*Degree+gender*Eigenvector+race*Eigenvector, data=applications_final)
summary(lm2)

# several interactions are omitted due to insufficient data/not statistically significant results:
# gender*race, all combinations are not statistically significant, probably due to having only 38 unique examiners in the dataset
# tenure*gender
# tenure*race
stargazer(lm1, lm2, 
          type="latex",
          dep.var.labels = "Application Processing Time",
          covariate.labels= c("Degree Centrality", "Eigenvector Centrality", "Tenure (days)", "Male", "Hispanic", "White", "Work Group 219", "Degree:Male","Degree:Hispanic","Degree:White","Eigenvector:Male","Eigenvector:Hispanic","Eigenvector:White"),
          digits = 2,
          font.size="LARGE")

```

Our baseline estimate is higher, around 1400 days.

Examiners are now expected to take less processing time with each additional advice-seeking, by 106 days.
Eigenvector centrality has a negative (longer) impact on processing time by 2117 days.(?)
With each additional day of tenure, female examiners shave about 0.05 days off their expected processing time.
Male examiners are expected to take about 110 days longer than their female counterparts.
Both hispanic and White examiners are expected to process faster than their asian colleagues.
As before, examiners from workgroup 219 appear to require much longer to process applications.

Among interaction terms, we expect male examiners to remove about 53 days of processing time when seeking advice (inc degree by 1) compared to women. 
- This would seem to imply that "importance" as measured by degree centrality is more meaningful for male examiners than it is for female examiners

Lets investigate that insight with some predictions:
```{r}
baseline = predict(lm2, data.frame(Degree=0,Eigenvector=0,tenure_days=0,gender='female',race='Asian',wg='162'))
lowDegMale = predict(lm2, data.frame(Degree=0,Eigenvector=0,tenure_days=0,gender='male',race='Asian',wg='162'))
lowDegFemale = baseline
highDegMale = predict(lm2, data.frame(Degree=5,Eigenvector=0,tenure_days=0,gender='male',race='Asian',wg='162'))
highDegFemale = predict(lm2, data.frame(Degree=5,Eigenvector=0,tenure_days=0,gender='female', race='Asian',wg='162'))

data.frame(baseline=baseline, unimportant_male=lowDegMale, important_male=highDegMale, unimportant_female=lowDegFemale, important_female=highDegFemale)

```

This affirms what we saw when examining the model summary: men seem to gain more benefit (in terms of reducing processing time) from advice seeking than women. We can't deduce why that is from this model, but conjecture might say that since this is a male-dominated organization, men seem to benefit (at least in terms of reducing processing time) from advice-seeking more than women.  

We can additionally see from the model summary that 